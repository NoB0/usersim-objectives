{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "This notebook contains the code to reproduce the experiments presented in the paper.\n",
    "\n",
    "Note that we focus on the dialogue policy and do not consider the other modules such as NLU and NLG of the conversational agent. The interaction model QRFA is used to build the dialogue policies for both the user simulators and the conversational agent. The user's actions are query and feedback, while the agent's actions are request and answer. Additionally, there is an action to finish the conversation that is shared by both.\n",
    "\n",
    "## Data\n",
    "\n",
    "For the experiments, we use the annotated datasets released along with the QRFA model. This choice is motivated by the fact that these datasets comprise different user behaviors to complete an information seeking task. Therefore, we assume a certain level of realism in the user simulators and conversational agents built using these datasets. The table below introduce the datasets.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each datasets, we build a user simulator and a conversational agent.\n",
    "Based one the idea of leave-one-out cross-validation, we study the implication relationships between the objectives of training and evaluation by considering the user population and agent associated to a dataset as the reference and the other user populations as simulated user populations. For each reference pair, we execute the following steps:\n",
    "\n",
    "1. Get transition probabilities from the reference user population and conversational agent.\n",
    "2. Train a success predicator for each simulated user population.\n",
    "3. Generate synthetic dialogues between the reference agent and the simulated user populations. Each dialogue is given a success score using scoring predictors.\n",
    "4. Compute metrics associated to the objectives of training and evaluation.\n",
    "5. Identify the best user simulator for training and evaluation based on the computed metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Tuple, Dict\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from statistics import mean, stdev\n",
    "\n",
    "ParticipantTransitionProbs = Dict[str, Dict[str, float]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome predictor\n",
    "\n",
    "The outcome predictor is defined as follows:\n",
    "\n",
    "$$\\hat{o} = \\frac{1}{1 + \\exp(-h(L, a^t_u, a^t_{CA}, p, i))}$$\n",
    "\n",
    "where $L$ is the length of the dialogue, $a^t_u$ and $a^t_{CA}$ are the actions of the user and the conversational agent at time $t$, $p$ is the patience, and $i$ is the inclination towards goal completion. The function $h$ is defined as follows:\n",
    "\n",
    "$$h(L, a^t_u, a^t_{CA}, p, i) = w_1 * \\frac{p}{L} + w_2 * \\tanh(i) * \\mathbb{1}(a^t_u = \\text{F}) + w_3 * \\mathbb{1}(a^t_{CA} = \\text{A})$$\n",
    "\n",
    "where $w_1$, $w_2$, $w_3$, and $w_4$ are the weights of the features and $\\mathbb{1}$ is the indicator function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dialogue_outcome(\n",
    "    patience: float,\n",
    "    inclination: float,\n",
    "    dialogue: List[str],\n",
    "    weights: List[float] = [1.0, 1.0, 0.5],\n",
    ") -> int:\n",
    "    \"\"\"Predicts the outcome of a dialogue.\n",
    "\n",
    "    Args:\n",
    "        patience: User's patience.\n",
    "        inclination: User's inclination towards goal completion.\n",
    "        dialogue: Dialogue to predict outcome for.\n",
    "        weights: Weights for the features.\n",
    "\n",
    "    Returns:\n",
    "        Outcome of the dialogue (0: failure, 1: success).\n",
    "    \"\"\"\n",
    "    last_user_action = None\n",
    "    last_agent_action = None\n",
    "\n",
    "    for action in reversed(dialogue):\n",
    "        if last_user_action is not None and last_agent_action is not None:\n",
    "            break\n",
    "\n",
    "        if action.startswith(\"U_\") and last_user_action is None:\n",
    "            last_user_action = 1.0 if \"F\" in action else 0.0\n",
    "        elif action.startswith(\"S_\") and last_agent_action is None:\n",
    "            last_agent_action = 1.0 if \"A\" in action else 0.0\n",
    "\n",
    "    if last_user_action is None:\n",
    "        last_user_action = 0.0\n",
    "    if last_agent_action is None:\n",
    "        last_agent_action = 0.0\n",
    "\n",
    "    features = [\n",
    "        patience / len(dialogue),\n",
    "        np.tanh(inclination) * last_user_action,\n",
    "        last_agent_action,\n",
    "    ]\n",
    "    h = sum([w * f for w, f in zip(weights, features)])\n",
    "    outcome_prob = 1 / (1 + np.exp(-h))\n",
    "    return 1 if outcome_prob >= 0.5 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User populations and conversational agents\n",
    "\n",
    "From the annotated dialogues, we can extract the transition probabilities for the user populations and conversational agents. The table summarizes the different user populations and conversational agents.\n",
    "\n",
    "| Dataset | User populations | Conversational agents |\n",
    "| ------- | ---------------- | --------------------- |\n",
    "| DSTC1   | U1               | A1                    |\n",
    "| DSTC2   | U2               | A2                    |\n",
    "| ODE     | U3               | A3                    |\n",
    "| SCS     | U4               | A4                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserPopulation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        patience: float,\n",
    "        inclination: float,\n",
    "        transition_probabilities: ParticipantTransitionProbs = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes a user population.\n",
    "\n",
    "        Args:\n",
    "            name: Name of the user population.\n",
    "            patience: User's patience.\n",
    "            inclination: User's inclination towards goal completion.\n",
    "            transition_probabilities: Transition probabilities. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.patience = patience\n",
    "        self.inclination = inclination\n",
    "        self.transition_probabilities = transition_probabilities\n",
    "\n",
    "    def add_historical_dialogues(self, dialogues: List[List[str]]) -> None:\n",
    "        \"\"\"Adds historical dialogues to the user population.\n",
    "\n",
    "        Args:\n",
    "            dialogues: List of dialogues.\n",
    "        \"\"\"\n",
    "        self.historical_dialogues = dialogues\n",
    "        self.historical_outcomes = [\n",
    "            predict_dialogue_outcome(self.patience, self.inclination, dialogue)\n",
    "            for dialogue in dialogues\n",
    "        ]\n",
    "\n",
    "    def get_user_actions(self) -> List[str]:\n",
    "        \"\"\"Returns the list of possible user actions.\"\"\"\n",
    "        user_actions = set()\n",
    "        for a_action in self.transition_probabilities.keys():\n",
    "            for u_action in self.transition_probabilities[a_action].keys():\n",
    "                user_actions.add(u_action)\n",
    "        return list(user_actions)\n",
    "\n",
    "    def get_agent_actions(self) -> List[str]:\n",
    "        \"\"\"Returns the list of possible agent actions.\"\"\"\n",
    "        return list(self.transition_probabilities.keys()) + [\"End\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dialogues(utterances: pd.DataFrame) -> List[List[str]]:\n",
    "    \"\"\"Preprocesses utterances to get dialogues.\n",
    "\n",
    "    Args:\n",
    "        utterances: All utterances in dataset.\n",
    "\n",
    "    Returns:\n",
    "        List of dialogues, each dialogue is a list of utterances.\n",
    "    \"\"\"\n",
    "    dialogues = []\n",
    "    case = 0\n",
    "    dialogue = []\n",
    "\n",
    "    for _, utterance in utterances.iterrows():\n",
    "        actions = np.unique(\n",
    "            [a[0] for a in utterance[\"new\"].split(\"+\")]\n",
    "        ).tolist()\n",
    "        if utterance[\"case ID\"] != case:\n",
    "            dialogues.append(dialogue)\n",
    "            dialogue = []\n",
    "            case = utterance[\"case ID\"]\n",
    "        elif \"Hello\" not in utterance[\"new\"] and \"Bye\" not in utterance[\"new\"]:\n",
    "            if len(dialogue) > 0 and dialogue[-1].startswith(\n",
    "                f\"{utterance['resource']}_\"\n",
    "            ):\n",
    "                prev_actions = [a[-1] for a in dialogue.pop(-1).split(\"+\")]\n",
    "                actions = prev_actions + actions\n",
    "\n",
    "            dialogue.append(\n",
    "                \"+\".join(\n",
    "                    [\n",
    "                        f\"{utterance['resource']}_{action[0]}\"\n",
    "                        for action in np.unique(actions)\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    dialogues = list(filter(None, dialogues))\n",
    "    return dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_probabilities(dialogues: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Get transition probabilities for a list of dialogues.\n",
    "\n",
    "    Args:\n",
    "        dialogues: Dialogues where each dialogue is a string of actions.\n",
    "\n",
    "    Returns:\n",
    "        Transition probabilities for each action in the dialogues.\n",
    "    \"\"\"\n",
    "    transitions = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        for i in range(len(dialogue) - 1):\n",
    "            current_action = dialogue[i]\n",
    "            next_action = dialogue[i + 1]\n",
    "            if i == 0:\n",
    "                transitions[\"Start\"][current_action] += 1\n",
    "\n",
    "            transitions[current_action][next_action] += 1\n",
    "\n",
    "        transitions[dialogue[-1]][\"End\"] += 1\n",
    "\n",
    "    probabilities = {}\n",
    "    for action in transitions.keys():\n",
    "        total = sum(transitions[action].values())\n",
    "        if total > 0:\n",
    "            probabilities[action] = {\n",
    "                next_action: count / total\n",
    "                for next_action, count in transitions[action].items()\n",
    "            }\n",
    "        else:\n",
    "            probabilities[action] = {}\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def get_participants_transition_probs(\n",
    "    transition_probs: Dict[str, float]\n",
    ") -> Tuple[ParticipantTransitionProbs, ParticipantTransitionProbs]:\n",
    "    \"\"\"Gets the transitions probabilities for each participant.\n",
    "\n",
    "    Args:\n",
    "        transition_probs: Transition probabilities for all actions.\n",
    "\n",
    "    Returns:\n",
    "        Transition probabilities for each participant.\n",
    "    \"\"\"\n",
    "    user_transition_probs = {}\n",
    "    agent_transition_probs = {}\n",
    "    for state, transition in transition_probs.items():\n",
    "        if state.startswith(\"U_\"):\n",
    "            agent_transition_probs[state] = transition\n",
    "        elif state.startswith(\"S_\"):\n",
    "            user_transition_probs[state] = transition\n",
    "\n",
    "    return user_transition_probs, agent_transition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_POPULATIONS = {}\n",
    "AGENT_POPULATIONS = {}\n",
    "\n",
    "datasets = [\n",
    "    (\n",
    "        \"U1\",\n",
    "        -0.9,\n",
    "        -0.9,\n",
    "        \"A1\",\n",
    "        \"data/annotated_datasets/1_dstc1_updated.csv\",\n",
    "    ),  # Impatient and critical user\n",
    "    (\n",
    "        \"U2\",\n",
    "        0.9,\n",
    "        -0.9,\n",
    "        \"A2\",\n",
    "        \"data/annotated_datasets/2_dstc2_updated.csv\",\n",
    "    ),  # Patient and critical user\n",
    "    (\n",
    "        \"U3\",\n",
    "        -0.9,\n",
    "        0.9,\n",
    "        \"A3\",\n",
    "        \"data/annotated_datasets/5_ode_updated.csv\",\n",
    "    ),  # Impatient and cooperative user\n",
    "    (\n",
    "        \"U4\",\n",
    "        0.9,\n",
    "        0.9,\n",
    "        \"A4\",\n",
    "        \"data/annotated_datasets/4_scs_updated.csv\",\n",
    "    ),  # Patient and cooperative user\n",
    "    (\n",
    "        \"U5\",\n",
    "        1e-5,\n",
    "        1e-5,\n",
    "        \"A5\",\n",
    "        \"data/annotated_datasets/6_mgshopdial_updated.csv\",\n",
    "    ),  # Neutral user\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/annotated_datasets/1_dstc1_updated.csv\n",
      "Processing data/annotated_datasets/2_dstc2_updated.csv\n",
      "Processing data/annotated_datasets/5_ode_updated.csv\n",
      "Processing data/annotated_datasets/4_scs_updated.csv\n",
      "Processing data/annotated_datasets/6_mgshopdial_updated.csv\n"
     ]
    }
   ],
   "source": [
    "data_stats = {}\n",
    "\n",
    "for user_pop, patience, inclination, agent, path in datasets:\n",
    "    print(f\"Processing {path}\")\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.dropna(subset=[\"new\"])\n",
    "    dialogues = preprocess_dialogues(data)\n",
    "\n",
    "    # Compute statistics on the dialogues: avg. # utterance and std dev\n",
    "    num_utterances = [len(dialogue) for dialogue in dialogues]\n",
    "    data_stats[f\"D({user_pop}, {agent})\"] = {\n",
    "        \"# dialogues\": len(dialogues),\n",
    "        \"Avg. # utterances\": mean(num_utterances),\n",
    "        \"Std. dev. # utterances\": stdev(num_utterances),\n",
    "    }\n",
    "\n",
    "    transition_probabilities = get_transition_probabilities(dialogues)\n",
    "    user_transition_probs, agent_transition_probs = (\n",
    "        get_participants_transition_probs(transition_probabilities)\n",
    "    )\n",
    "\n",
    "    population = UserPopulation(\n",
    "        user_pop, patience, inclination, user_transition_probs\n",
    "    )\n",
    "    population.add_historical_dialogues(dialogues)\n",
    "    USER_POPULATIONS[user_pop] = population\n",
    "\n",
    "    AGENT_POPULATIONS[agent] = {\n",
    "        \"transition_probabilities\": agent_transition_probs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dialogues statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_fb793\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fb793_level0_col0\" class=\"col_heading level0 col0\" ># dialogues</th>\n",
       "      <th id=\"T_fb793_level0_col1\" class=\"col_heading level0 col1\" >Avg. # utterances</th>\n",
       "      <th id=\"T_fb793_level0_col2\" class=\"col_heading level0 col2\" >Std. dev. # utterances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fb793_level0_row0\" class=\"row_heading level0 row0\" >D(U1, A1)</th>\n",
       "      <td id=\"T_fb793_row0_col0\" class=\"data row0 col0\" >15577.000</td>\n",
       "      <td id=\"T_fb793_row0_col1\" class=\"data row0 col1\" >24.217</td>\n",
       "      <td id=\"T_fb793_row0_col2\" class=\"data row0 col2\" >22.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb793_level0_row1\" class=\"row_heading level0 row1\" >D(U2, A2)</th>\n",
       "      <td id=\"T_fb793_row1_col0\" class=\"data row1 col0\" >2117.000</td>\n",
       "      <td id=\"T_fb793_row1_col1\" class=\"data row1 col1\" >10.171</td>\n",
       "      <td id=\"T_fb793_row1_col2\" class=\"data row1 col2\" >4.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb793_level0_row2\" class=\"row_heading level0 row2\" >D(U3, A3)</th>\n",
       "      <td id=\"T_fb793_row2_col0\" class=\"data row2 col0\" >25.000</td>\n",
       "      <td id=\"T_fb793_row2_col1\" class=\"data row2 col1\" >15.000</td>\n",
       "      <td id=\"T_fb793_row2_col2\" class=\"data row2 col2\" >8.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb793_level0_row3\" class=\"row_heading level0 row3\" >D(U4, A4)</th>\n",
       "      <td id=\"T_fb793_row3_col0\" class=\"data row3 col0\" >38.000</td>\n",
       "      <td id=\"T_fb793_row3_col1\" class=\"data row3 col1\" >1.579</td>\n",
       "      <td id=\"T_fb793_row3_col2\" class=\"data row3 col2\" >0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb793_level0_row4\" class=\"row_heading level0 row4\" >D(U5, A5)</th>\n",
       "      <td id=\"T_fb793_row4_col0\" class=\"data row4 col0\" >63.000</td>\n",
       "      <td id=\"T_fb793_row4_col1\" class=\"data row4 col1\" >20.159</td>\n",
       "      <td id=\"T_fb793_row4_col2\" class=\"data row4 col2\" >9.474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1624ecfa0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data_stats).transpose().style.format(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    data,\n",
    "    dialogues,\n",
    "    num_utterances,\n",
    "    transition_probabilities,\n",
    "    user_transition_probs,\n",
    "    agent_transition_probs,\n",
    "    data_stats,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of synthetic dialogues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_action(\n",
    "    current_action: str, transition_probs: ParticipantTransitionProbs\n",
    ") -> str:\n",
    "    \"\"\"Samples the next action based on transition probabilities.\n",
    "\n",
    "    Args:\n",
    "        current_action: Current action.\n",
    "        transition_probs: Transition probabilities.\n",
    "\n",
    "    Returns:\n",
    "        Next action.\n",
    "    \"\"\"\n",
    "    next_actions = list(transition_probs[current_action].keys())\n",
    "    probabilities = list(transition_probs[current_action].values())\n",
    "    sampled_action = np.random.choice(next_actions, p=probabilities)\n",
    "    return sampled_action\n",
    "\n",
    "\n",
    "def sample_dialogue(\n",
    "    agent_transition_probs: ParticipantTransitionProbs,\n",
    "    user_transition_probs: ParticipantTransitionProbs,\n",
    ") -> List[str]:\n",
    "    \"\"\"Samples a dialogue.\n",
    "\n",
    "    Args:\n",
    "        agent_transition_probs: Transition probabilities for the agent.\n",
    "        user_transition_probs: Transition probabilities for the user.\n",
    "\n",
    "    Returns:\n",
    "        Dialogue as list of actions.\n",
    "    \"\"\"\n",
    "    dialogue = []\n",
    "    is_finished = False\n",
    "\n",
    "    current_action = random.choice(\n",
    "        list(agent_transition_probs.keys())\n",
    "        + list(user_transition_probs.keys())\n",
    "    )\n",
    "    dialogue.append(current_action)\n",
    "    while not is_finished:\n",
    "        try:\n",
    "            if current_action.startswith(\"U_\"):\n",
    "                current_action = sample_next_action(\n",
    "                    current_action, agent_transition_probs\n",
    "                )\n",
    "            else:\n",
    "                current_action = sample_next_action(\n",
    "                    current_action, user_transition_probs\n",
    "                )\n",
    "            if current_action == \"End\":\n",
    "                is_finished = True\n",
    "                break\n",
    "            dialogue.append(current_action)\n",
    "        except KeyError:\n",
    "            current_action = current_action.split(\"+\")[-1]\n",
    "\n",
    "    return dialogue\n",
    "\n",
    "\n",
    "def sample_dialogues(\n",
    "    agent_transition_probs: ParticipantTransitionProbs,\n",
    "    user_transition_probs: ParticipantTransitionProbs,\n",
    "    num_dialogues: int,\n",
    "    patience: float,\n",
    "    inclination: float,\n",
    ") -> List[Tuple[List[str], bool]]:\n",
    "    \"\"\"Samples dialogues.\n",
    "\n",
    "    Args:\n",
    "        agent_transition_probs: Transition probabilities for the agent.\n",
    "        user_transition_probs: Transition probabilities for the user.\n",
    "        num_dialogues: Number of dialogues to sample.\n",
    "        patience: User's patience.\n",
    "        inclination: User's inclination towards goal completion.\n",
    "\n",
    "    Returns:\n",
    "        Dialogues with success status.\n",
    "    \"\"\"\n",
    "    dialogues = []\n",
    "    for _ in range(num_dialogues):\n",
    "        dialogue = sample_dialogue(\n",
    "            agent_transition_probs, user_transition_probs\n",
    "        )\n",
    "\n",
    "        success = predict_dialogue_outcome(patience, inclination, dialogue)\n",
    "        dialogues.append((dialogue, success))\n",
    "\n",
    "    return dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "This part contains the methods to compute the metrics associated to the training and evaluation objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from rouge_score import rouge_scorer\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We choose to use Jensen-Shannon divergence (JSD) and ROUGE-L as metrics to assess the similarity between the user population and simulated user populations. These allow us to make an assessment at the utterance- and dialogue-level respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jsd(\n",
    "    user_policy: ParticipantTransitionProbs,\n",
    "    simulated_user_policy: ParticipantTransitionProbs,\n",
    ") -> float:\n",
    "    \"\"\"Computes Jensen-Shannon divergence between user and simulated user\n",
    "    policies.\n",
    "\n",
    "    It computes the Jensen-Shannon divergence between the transition\n",
    "    probabilities for each state and then averages them. Epsilon is added to\n",
    "    avoid division by zero.\n",
    "\n",
    "    Args:\n",
    "        user_policy: User policy.\n",
    "        simulated_user_policy: Simulated user policy.\n",
    "\n",
    "    Returns:\n",
    "        Jensen-Shannon divergence.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-9\n",
    "    total_jsd = 0.0\n",
    "    for state, transitions_probabilities in user_policy.items():\n",
    "        # Add epsilon to avoid division by zero\n",
    "        simulated_user_policy[state] = {\n",
    "            k: simulated_user_policy.get(state, {}).get(k, epsilon)\n",
    "            for k in transitions_probabilities.keys()\n",
    "        }\n",
    "\n",
    "        probabilities = np.array(list(transitions_probabilities.values()))\n",
    "        simulated_probabilities = np.array(\n",
    "            list(simulated_user_policy[state].values())\n",
    "        )\n",
    "\n",
    "        total_jsd += distance.jensenshannon(\n",
    "            probabilities, simulated_probabilities, base=2\n",
    "        )\n",
    "    return total_jsd / len(user_policy.keys())\n",
    "\n",
    "\n",
    "def compute_rouge_score(\n",
    "    historical_dialogues: List[List[str]], simulated_dialogues: List[List[str]]\n",
    ") -> float:\n",
    "    \"\"\"Computes ROUGE-L score between historical and simulated dialogues.\n",
    "\n",
    "    It computes the average ROUGE-L score between all pairs of historical and\n",
    "    simulated dialogues.\n",
    "\n",
    "    Args:\n",
    "        historical_dialogues: Historical dialogues.\n",
    "        simulated_dialogues: Simulated dialogues.\n",
    "\n",
    "    Returns:\n",
    "        ROUGE-L score.\n",
    "    \"\"\"\n",
    "    historical_dialogues = [\" \".join(d) for d in historical_dialogues]\n",
    "    simulated_dialogues = [\" \".join(d) for d in simulated_dialogues]\n",
    "    total_score = 0.0\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"])\n",
    "    pairs = list(product(historical_dialogues, simulated_dialogues))\n",
    "    for h, s in pairs:\n",
    "        total_score += scorer.score(h, s)[\"rougeL\"].fmeasure\n",
    "    return total_score / len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We use the success rate as the performance metric to evaluate the conversational agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_success_rate(successes: List[int]) -> float:\n",
    "    \"\"\"Computes success rate.\n",
    "\n",
    "    Args:\n",
    "        successes: Successes.\n",
    "\n",
    "    Returns:\n",
    "        Success rate.\n",
    "    \"\"\"\n",
    "    return sum(successes) / len(successes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out cross-validation\n",
    "\n",
    "In this part, we perform a leave-out-one out experiment to answer the following questions: is the optimal user simulator for training also the best for evaluation, and vice versa?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_pairs = [\n",
    "    (\"U1\", \"A1\"),\n",
    "    (\"U2\", \"A2\"),\n",
    "    (\"U3\", \"A3\"),\n",
    "    (\"U4\", \"A4\"),\n",
    "    (\"U5\", \"A5\"),\n",
    "]\n",
    "# participant_pairs = [(\"U2\", \"A2\"), (\"U3\", \"A3\"), (\"U4\", \"A4\"), (\"U5\", \"A5\")]\n",
    "num_synthetic_dialogues = 500\n",
    "\n",
    "results = defaultdict(dict)\n",
    "\n",
    "for user_pop, agent in participant_pairs:\n",
    "    print(f\"Reference - {user_pop}, {agent}\")\n",
    "    user_population = USER_POPULATIONS[user_pop]\n",
    "    historical_success_rate = compute_success_rate(\n",
    "        user_population.historical_outcomes\n",
    "    )\n",
    "    for _, simulated_user_population in USER_POPULATIONS.items():\n",
    "        if user_pop == simulated_user_population.name:\n",
    "            continue\n",
    "\n",
    "        print(\n",
    "            f\"{time.ctime()} - Simulated user population: {simulated_user_population.name}\"\n",
    "        )\n",
    "\n",
    "        # Generate synthetic dialogues\n",
    "        synthetic_dialogues_data = sample_dialogues(\n",
    "            AGENT_POPULATIONS[agent][\"transition_probabilities\"],\n",
    "            simulated_user_population.transition_probabilities,\n",
    "            num_synthetic_dialogues,\n",
    "            simulated_user_population.patience,\n",
    "            simulated_user_population.inclination,\n",
    "        )\n",
    "\n",
    "        simulated_dialogues = []\n",
    "        simulated_dialogues_success = []\n",
    "        for dialogue, success in synthetic_dialogues_data:\n",
    "            simulated_dialogues.append(dialogue)\n",
    "            simulated_dialogues_success.append(success)\n",
    "\n",
    "        # Compute ROUGE-L score\n",
    "        rouge_l_score = compute_rouge_score(\n",
    "            user_population.historical_dialogues,\n",
    "            simulated_dialogues,\n",
    "        )\n",
    "\n",
    "        # Compute success rate\n",
    "        success_rate = compute_success_rate(simulated_dialogues_success)\n",
    "\n",
    "        # Absolute difference success rate\n",
    "        abs_diff_success_rate = abs(success_rate - historical_success_rate)\n",
    "\n",
    "        results[f\"{user_pop}, {agent}\"][simulated_user_population.name] = {\n",
    "            \"ROUGE-L\": rouge_l_score,\n",
    "            \"Success rate\": success_rate,\n",
    "            \"Abs. diff. success rate\": abs_diff_success_rate,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for participant_pair, d in results.items():\n",
    "    for simulated_user, metrics in d.items():\n",
    "        rows.append(\n",
    "            (\n",
    "                participant_pair,\n",
    "                simulated_user,\n",
    "                *(value for _, value in metrics.items()),\n",
    "            )\n",
    "        )\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\n",
    "        \"Reference\",\n",
    "        \"Simulated user pop.\",\n",
    "        \"ROUGE-L\",\n",
    "        \"Success rate\",\n",
    "        \"Abs. diff. success rate\",\n",
    "    ],\n",
    ")\n",
    "summary.set_index([\"Reference\", \"Simulated user pop.\"], inplace=True)\n",
    "\n",
    "summary.style.format(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jensen-Shannon divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsd_results = defaultdict(dict)\n",
    "\n",
    "for user_pop1, user_pop2 in product(USER_POPULATIONS.keys(), repeat=2):\n",
    "    if user_pop1 != user_pop2:\n",
    "        user_policy1 = USER_POPULATIONS[user_pop1].transition_probabilities\n",
    "        user_policy2 = USER_POPULATIONS[user_pop2].transition_probabilities\n",
    "        jsd = compute_jsd(user_policy1, user_policy2)\n",
    "        jsd_results[user_pop1][user_pop2] = jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(jsd_results).sort_index().style.format(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra experiments\n",
    "\n",
    "In this part, we perform an additional experiment to verify that an agent trained with the optimal user simulator gets a better success rate than the untrained agent when used with the reference user population.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom environment\n",
    "\n",
    "We create a custom OpenAI Gym environment to train the conversational agent. The environment comprises a user simulator that converse with the conversational agent and provide feedback. It is important to note that the simulated user is always the one starting the conversation.\n",
    "\n",
    "#### Action space\n",
    "\n",
    "The conversational agent can take four actions:\n",
    "\n",
    "- Answer: The conversational agent answers the user simulator's query.\n",
    "- Request: The conversational agent requests information from the user simulator.\n",
    "- Answer+Request: Combines the request and answer actions.\n",
    "- End: The conversational agent finishes the conversation.\n",
    "\n",
    "#### Observation space\n",
    "\n",
    "For simplicity, the observation space comprises the user simulator's possible actions:\n",
    "\n",
    "- Feedback: The user simulator provides feedback to the conversational agent.\n",
    "- Query: The user simulator requests information from the conversational agent.\n",
    "- Feedback+Query: Combines the query and feedback actions.\n",
    "- End: The user simulator finishes the conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalEnv(gym.Env):\n",
    "    \"\"\"Custom environment to trained a dialogue policy using a user simulator.\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"console\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent_actions: List[str],\n",
    "        user_actions: List[str],\n",
    "        simulated_user_population: UserPopulation,\n",
    "        max_nb_utterances: int = 100,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the conversational environment.\n",
    "\n",
    "        Args:\n",
    "            agent_actions: List of possible actions for the agent.\n",
    "            user_actions: List of possible actions for the user.\n",
    "            simulated_user_population: Simulated user population.\n",
    "            max_nb_utterances: Maximum number of utterances in the dialogue.\n",
    "              Defaults to 100.\n",
    "        \"\"\"\n",
    "        super(ConversationalEnv, self).__init__()\n",
    "\n",
    "        self.agent_possible_actions = agent_actions\n",
    "        self.user_possible_actions = user_actions\n",
    "\n",
    "        # Define the action and observation space\n",
    "        self.action_space = spaces.Discrete(len(self.agent_possible_actions))\n",
    "        self.observation_space = spaces.Discrete(\n",
    "            len(self.user_possible_actions)\n",
    "        )\n",
    "\n",
    "        self.user = simulated_user_population\n",
    "        self.max_nb_utterances = max_nb_utterances\n",
    "\n",
    "        self.agent_action = None\n",
    "        self.user_action = None\n",
    "        self.dialogue = []\n",
    "\n",
    "    def reset(self, seed: Optional[int] = 0) -> Tuple[int, Dict]:\n",
    "        \"\"\"Resets the environment and returns the initial observation.\n",
    "\n",
    "        Args:\n",
    "            seed: Random seed to use for reproducibility.\n",
    "\n",
    "        Returns:\n",
    "            Initial observation and additional information.\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        self.agent_action = None\n",
    "        u_act = \"End\"\n",
    "        while u_act == \"End\":\n",
    "            u_act = random.choice(self.user_possible_actions)\n",
    "        self.user_action = u_act\n",
    "        self.dialogue = [self.user_action]\n",
    "\n",
    "        return self.user_possible_actions.index(self.user_action), {}\n",
    "\n",
    "    def _compute_reward(self) -> Tuple[float, bool]:\n",
    "        \"\"\"Computes the reward based on the success probability.\n",
    "\n",
    "        Args:\n",
    "            truncated: Flag indicating if the conversation was truncated, i.e.,\n",
    "              unsuccessfully terminated.\n",
    "        Returns:\n",
    "            Reward value and success flag.\n",
    "        \"\"\"\n",
    "        if self.agent_action == \"End\" or self.user_action == \"End\":\n",
    "            b_success = predict_dialogue_outcome(\n",
    "                self.user.patience, self.user.inclination, self.dialogue\n",
    "            )\n",
    "            return (1.0, True) if b_success else (-1.0, False)\n",
    "        return -0.1, None\n",
    "\n",
    "    def step(self, action: int) -> Tuple:\n",
    "        \"\"\"Executes the agent action and returns the next user action.\n",
    "\n",
    "        Args:\n",
    "            action: Action selected by the agent.\n",
    "\n",
    "        Returns:\n",
    "            Tuple with the next user action (observation), reward, termination\n",
    "              flag, truncation flag, and additional information.\n",
    "        \"\"\"\n",
    "        self.agent_action = self.agent_possible_actions[action]\n",
    "        if self.agent_action == \"End\":\n",
    "            # End the conversation\n",
    "            is_terminated = True\n",
    "            is_truncated = False\n",
    "            reward, b_success = self._compute_reward()\n",
    "            return (\n",
    "                None,\n",
    "                reward,\n",
    "                is_terminated,\n",
    "                is_truncated,\n",
    "                {\"success\": b_success},\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            self.user_action = sample_next_action(\n",
    "                self.agent_action, self.user.transition_probabilities\n",
    "            )\n",
    "        except KeyError:\n",
    "            # All user population do not have the combined action\n",
    "            self.user_action = sample_next_action(\n",
    "                self.agent_action.split(\"+\")[-1],\n",
    "                self.user.transition_probabilities,\n",
    "            )\n",
    "\n",
    "        self.dialogue.extend([self.agent_action, self.user_action])\n",
    "\n",
    "        # Check if the conversation is terminated or truncated\n",
    "        is_truncated = len(self.dialogue) >= self.max_nb_utterances\n",
    "        is_terminated = self.agent_action == \"End\" or self.user_action == \"End\"\n",
    "\n",
    "        # Compute the reward\n",
    "        reward, b_success = self._compute_reward()\n",
    "\n",
    "        next_observation = self.user_possible_actions.index(self.user_action)\n",
    "        return (\n",
    "            next_observation,\n",
    "            reward,\n",
    "            is_terminated,\n",
    "            is_truncated,\n",
    "            {\"success\": b_success},\n",
    "        )\n",
    "\n",
    "    def render(self, mode: str = \"console\") -> None:\n",
    "        \"\"\"Renders the current state of the environment.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: If the rendering mode is not supported.\n",
    "        \"\"\"\n",
    "        if mode != \"console\":\n",
    "            raise NotImplementedError(\"Only console rendering is supported.\")\n",
    "        print(f\"Dialogue: {self.dialogue}\")\n",
    "\n",
    "    def close(self) -> List[str]:\n",
    "        \"\"\"Cleans up the environment.\n",
    "\n",
    "        Returns:\n",
    "            The dialogue history.\n",
    "        \"\"\"\n",
    "        return self.dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(env_kwargs: Dict[str, Any]) -> ConversationalEnv:\n",
    "    \"\"\"Creates a single environment instance.\n",
    "\n",
    "    Args:\n",
    "        env_kwargs: Keyword arguments to pass to the environment constructor.\n",
    "\n",
    "    Returns:\n",
    "        Environment.\n",
    "    \"\"\"\n",
    "    return ConversationalEnv(**env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Environment validation and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = create_env(\n",
    "    {\n",
    "        \"agent_actions\": [\"S_R\", \"S_A\", \"S_A+S_R\", \"End\"],\n",
    "        \"user_actions\": [\"U_F\", \"U_Q\", \"U_F+U_Q\", \"End\"],\n",
    "        \"simulated_user_population\": USER_POPULATIONS[\"U1\"],\n",
    "        \"max_nb_utterances\": 30,\n",
    "    }\n",
    ")\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\n",
    "    f\"Observation space: {env.observation_space}\\nAction space: \"\n",
    "    f\"{env.action_space}\"\n",
    ")\n",
    "\n",
    "n_steps = 10\n",
    "for _ in range(n_steps):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if done or truncated:\n",
    "        env.render()\n",
    "        print(f\"Episode finished - Reward: {reward}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "env_kwargs = {\n",
    "    \"agent_actions\": [\"S_R\", \"S_A\", \"S_A+S_R\", \"End\"],\n",
    "    \"user_actions\": [\"U_F\", \"U_Q\", \"U_F+U_Q\", \"End\"],\n",
    "    \"simulated_user_population\": USER_POPULATIONS[\"U1\"],\n",
    "    \"max_nb_utterances\": 30,\n",
    "}\n",
    "n_envs = 3\n",
    "envs = DummyVecEnv([lambda: create_env(env_kwargs) for _ in range(n_envs)])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, verbose=1).learn(25_000)\n",
    "\n",
    "env = create_env(env_kwargs)\n",
    "obs = env.reset()[0]\n",
    "n_steps = 10\n",
    "for _ in range(n_steps):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if done or truncated:\n",
    "        env.render()\n",
    "        print(f\"Episode finished - Reward: {reward}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of conversational agents\n",
    "\n",
    "We train conversational agents using different user simulators. The training process is based on the PPO algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import (\n",
    "    EvalCallback,\n",
    "    StopTrainingOnNoModelImprovement,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "    env_kwargs: Dict[str, Any],\n",
    "    population_name: str,\n",
    "    n_envs: int,\n",
    "    n_steps: int = int(1e10),\n",
    ") -> PPO:\n",
    "    \"\"\"Trains an agent using the PPO algorithm.\n",
    "\n",
    "    Args:\n",
    "        env_kwargs: Keyword arguments to pass to the environment constructor.\n",
    "        name: Name of the simulated user population.\n",
    "        n_envs: Number of parallel environments.\n",
    "        n_steps: Number of training steps. Defaults to 1e10.\n",
    "\n",
    "    Returns:\n",
    "        Trained agent.\n",
    "    \"\"\"\n",
    "    envs = DummyVecEnv([lambda: create_env(env_kwargs) for _ in range(n_envs)])\n",
    "\n",
    "    stop_train_callback = StopTrainingOnNoModelImprovement(\n",
    "        max_no_improvement_evals=3, min_evals=5, verbose=1\n",
    "    )\n",
    "\n",
    "    # Eval environment\n",
    "    eval_env = create_env(env_kwargs)\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=f\"./models/{population_name}/\",\n",
    "        log_path=\"./logs\",\n",
    "        eval_freq=500,\n",
    "        callback_after_eval=stop_train_callback,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "    )\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", envs, tensorboard_log=\"./logs\")\n",
    "    model.learn(n_steps, callback=eval_callback, tb_log_name=population_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training plots\n",
    "\n",
    "Start TensorBoard to check the training plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_POLICIES = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pop_name, user_pop in USER_POPULATIONS.items():\n",
    "    print(f\"Training agent for {pop_name}\")\n",
    "    env_kwargs = {\n",
    "        \"agent_actions\": [\"S_R\", \"S_A\", \"S_A+S_R\", \"End\"],\n",
    "        \"user_actions\": [\"U_F\", \"U_Q\", \"U_F+U_Q\", \"End\"],\n",
    "        \"simulated_user_population\": user_pop,\n",
    "        \"max_nb_utterances\": 35,\n",
    "    }\n",
    "    model = train_agent(env_kwargs, pop_name, n_envs=3, n_steps=5000)\n",
    "    TRAINED_POLICIES[f\"A*_{pop_name}\"] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute success rate\n",
    "\n",
    "For each trained conversational agent, we compute the success rate when interacting with the user populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_success_rate = defaultdict(lambda: defaultdict(list))\n",
    "num_eval_episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    for agent_name, model in TRAINED_POLICIES.items():\n",
    "        for pop_name, user_pop in USER_POPULATIONS.items():\n",
    "            env_kwargs = {\n",
    "                \"agent_actions\": [\"S_R\", \"S_A\", \"S_A+S_R\", \"End\"],\n",
    "                \"user_actions\": [\"U_F\", \"U_Q\", \"U_F+U_Q\", \"End\"],\n",
    "                \"simulated_user_population\": user_pop,\n",
    "                \"max_nb_utterances\": 30,\n",
    "            }\n",
    "            env = create_env(env_kwargs)\n",
    "            obs = env.reset()[0]\n",
    "            successes = []\n",
    "            for _ in range(num_eval_episodes):\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, reward, done, truncated, info = env.step(action)\n",
    "                if done or truncated:\n",
    "                    successes.append(int(info[\"success\"] == True))\n",
    "                    obs = env.reset()[0]\n",
    "            success_rate = compute_success_rate(successes)\n",
    "            agents_success_rate[agent_name][pop_name].append(success_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({k: {kk: f\"{round(mean(vv), 3)} +/- {round(stdev(vv),3)}\" for kk, vv in v.items()} for k, v in agents_success_rate.items()}).style.format(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical analysis\n",
    "\n",
    "1. Perform Kruskal-Wallis H-test to check if there is a significant difference between the success rates of the user populations.\n",
    "2. Perform Mann-Withney U-test for pairwise comparisons between the user populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "sst_results = defaultdict(dict)\n",
    "alpha = 0.05\n",
    "\n",
    "for user_pop in USER_POPULATIONS.keys():\n",
    "    success_rates = []\n",
    "    for agent_name, success_rate in agents_success_rate.items():\n",
    "        success_rates.append(success_rate[user_pop])\n",
    "    kruskal_result = stats.kruskal(*success_rates)\n",
    "    sst_results[user_pop][\"Kruskal\"] = kruskal_result.pvalue\n",
    "\n",
    "    if kruskal_result.pvalue < alpha:\n",
    "        for i in range(len(success_rates)-1):\n",
    "            for j in range(i+1, len(success_rates)):\n",
    "                mannwhitney_result = stats.mannwhitneyu(\n",
    "                    success_rates[i], success_rates[j]\n",
    "                )\n",
    "                sst_results[user_pop][f\"Mann-Whitney {i+1}-{j+1}\"] = mannwhitney_result.pvalue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sst_results).style.format(precision=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigir24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
