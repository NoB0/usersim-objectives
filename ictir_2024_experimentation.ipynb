{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical analysis ICTIR '24\n",
    "\n",
    "This notebook contains the code to reproduce the experiments presented in the paper.\n",
    "\n",
    "Note that we focus on the dialogue policy and do not consider the other modules such as NLU and NLG of the conversational agent. The interaction model QRFA is used to build the dialogue policies for both the user simulators and the conversational agent. The user's actions are query and feedback, while the agent's actions are request and answer. Additionally, there is an action to finish the conversation that is shared by both.\n",
    "\n",
    "## Data\n",
    "\n",
    "For the experiments, we use the annotated datasets released along with the QRFA model. This choice is motivated by the fact that these datasets comprise different user behaviors to complete an information seeking task. Therefore, we assume a certain level of realism in the user simulators and conversational agents built using these datasets. The table below introduce the datasets.\n",
    "\n",
    "| Dataset | # Dialogues |\n",
    "| ------- | ----------- |\n",
    "| DSTC1   | 15,577      |\n",
    "| DSTC2   | 2,117       |\n",
    "| ODE     | 25          |\n",
    "| SCS     | 38          |\n",
    "\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each datasets, we build a user simulator and a conversational agent.\n",
    "Based one the idea of leave-one-out cross-validation, we study the implication relationships between the objectives of training and evaluation by considering the user population and agent associated to a dataset as the reference and the other user populations as simulated user populations. For each reference pair, we execute the following steps:\n",
    "\n",
    "1. Get transition probabilities from the reference user population and conversational agent.\n",
    "2. Train a success predicator for each simulated user population.\n",
    "3. Generate synthetic dialogues between the reference agent and the simulated user populations. Each dialogue is given a success score using scoring predictors.\n",
    "4. Compute metrics associated to the objectives of training and evaluation.\n",
    "5. Identify the best user simulator for training and evaluation based on the computed metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (0.70.0)\n",
      "Requirement already satisfied: numpy in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (1.26.2)\n",
      "Requirement already satisfied: requests in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (4.66.1)\n",
      "Requirement already satisfied: regex in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (2023.10.3)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (4.39.1)\n",
      "Requirement already satisfied: datasets in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (2.15.0)\n",
      "Requirement already satisfied: scipy in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (1.3.2)\n",
      "Requirement already satisfied: seqeval in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (1.2.2)\n",
      "Requirement already satisfied: tensorboard in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (2.15.1)\n",
      "Requirement already satisfied: tensorboardx in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (2.6.2.2)\n",
      "Requirement already satisfied: pandas in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (2.1.3)\n",
      "Requirement already satisfied: tokenizers in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (0.15.2)\n",
      "Requirement already satisfied: wandb>=0.10.32 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (0.16.4)\n",
      "Requirement already satisfied: streamlit in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (1.32.2)\n",
      "Requirement already satisfied: sentencepiece in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from simpletransformers) (0.1.99)\n",
      "Requirement already satisfied: filelock in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from transformers>=4.31.0->simpletransformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from transformers>=4.31.0->simpletransformers) (0.19.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from transformers>=4.31.0->simpletransformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from transformers>=4.31.0->simpletransformers) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from transformers>=4.31.0->simpletransformers) (0.4.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from wandb>=0.10.32->simpletransformers) (3.1.40)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from wandb>=0.10.32->simpletransformers) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from wandb>=0.10.32->simpletransformers) (1.43.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from wandb>=0.10.32->simpletransformers) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from wandb>=0.10.32->simpletransformers) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from wandb>=0.10.32->simpletransformers) (4.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from wandb>=0.10.32->simpletransformers) (4.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from requests->simpletransformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from requests->simpletransformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from requests->simpletransformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from requests->simpletransformers) (2023.11.17)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from datasets->simpletransformers) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from datasets->simpletransformers) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from datasets->simpletransformers) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from datasets->simpletransformers) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from datasets->simpletransformers) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->simpletransformers) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from datasets->simpletransformers) (3.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from pandas->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from pandas->simpletransformers) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from pandas->simpletransformers) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from scikit-learn->simpletransformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from scikit-learn->simpletransformers) (3.2.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from streamlit->simpletransformers) (5.2.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from streamlit->simpletransformers) (1.7.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from streamlit->simpletransformers) (5.3.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from streamlit->simpletransformers) (10.1.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from streamlit->simpletransformers) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from streamlit->simpletransformers) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from streamlit->simpletransformers) (0.8.1b0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from streamlit->simpletransformers) (6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from tensorboard->simpletransformers) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from tensorboard->simpletransformers) (1.59.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from tensorboard->simpletransformers) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from tensorboard->simpletransformers) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from tensorboard->simpletransformers) (3.5.1)\n",
      "Requirement already satisfied: six>1.9 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from tensorboard->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from tensorboard->simpletransformers) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from tensorboard->simpletransformers) (3.0.1)\n",
      "Requirement already satisfied: jinja2 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.21.1)\n",
      "Requirement already satisfied: toolz in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from aiohttp->datasets->simpletransformers) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from aiohttp->datasets->simpletransformers) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from aiohttp->datasets->simpletransformers) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from aiohttp->datasets->simpletransformers) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->simpletransformers) (6.8.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->simpletransformers) (3.17.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->simpletransformers) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from statistics import mean, stdev\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import (\n",
    "    ClassificationModel,\n",
    "    ClassificationArgs,\n",
    ")\n",
    "\n",
    "\n",
    "ParticipantTransitionProbs = Dict[str, Dict[str, float]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User populations and conversational agents\n",
    "\n",
    "From the annotated dialogues, we can extract the transition probabilities for the user populations and conversational agents. The table summarizes the different user populations and conversational agents.\n",
    "\n",
    "| Dataset | User populations | Conversational agents |\n",
    "| ------- | ---------------- | --------------------- |\n",
    "| DSTC1   | U1               | A1                    |\n",
    "| DSTC2   | U2               | A2                    |\n",
    "| ODE     | U3               | A3                    |\n",
    "| SCS     | U4               | A4                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserPopulation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        transition_probabilities: ParticipantTransitionProbs = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes a user population.\n",
    "\n",
    "        Agrs:\n",
    "            name: Name of the user population.\n",
    "            transition_probabilities: Transition probabilities. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.transition_probabilities = transition_probabilities\n",
    "\n",
    "    def add_historical_dialogues(self, dialogues: List[List[str]]) -> None:\n",
    "        \"\"\"Adds historical dialogues to the user population.\n",
    "\n",
    "        Args:\n",
    "            dialogues: List of dialogues.\n",
    "        \"\"\"\n",
    "        self.historical_dialogues = dialogues\n",
    "\n",
    "    def get_user_actions(self) -> List[str]:\n",
    "        \"\"\"Returns the list of possible user actions.\"\"\"\n",
    "        user_actions = set()\n",
    "        for a_action in self.transition_probabilities.keys():\n",
    "            for u_action in self.transition_probabilities[a_action].keys():\n",
    "                user_actions.add(u_action)\n",
    "        return list(user_actions)\n",
    "\n",
    "    def get_agent_actions(self) -> List[str]:\n",
    "        \"\"\"Returns the list of possible agent actions.\"\"\"\n",
    "        return list(self.transition_probabilities.keys()) + [\"End\"]\n",
    "\n",
    "    def set_success_predictor(self, success_predictor: ClassificationModel) -> None:\n",
    "        \"\"\"Sets the success predictor.\n",
    "\n",
    "        Args:\n",
    "            success_predictor: Success predictor model.\n",
    "        \"\"\"\n",
    "        self.success_predictor = success_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dialogues(utterances: pd.DataFrame) -> List[List[str]]:\n",
    "    \"\"\"Preprocesses utterances to get dialogues.\n",
    "\n",
    "    Args:\n",
    "        utterances: All utterances in dataset.\n",
    "\n",
    "    Returns:\n",
    "        List of dialogues, each dialogue is a list of utterances.\n",
    "    \"\"\"\n",
    "    dialogues = []\n",
    "    case = 0\n",
    "    dialogue = []\n",
    "\n",
    "    for _, utterance in utterances.iterrows():\n",
    "        actions = np.unique(\n",
    "            [a[0] for a in utterance[\"new\"].split(\"+\")]\n",
    "        ).tolist()\n",
    "        if utterance[\"case ID\"] != case:\n",
    "            dialogues.append(dialogue)\n",
    "            dialogue = []\n",
    "            case = utterance[\"case ID\"]\n",
    "        elif \"Hello\" not in utterance[\"new\"] and \"Bye\" not in utterance[\"new\"]:\n",
    "            if len(dialogue) > 0 and dialogue[-1].startswith(\n",
    "                f\"{utterance['resource']}_\"\n",
    "            ):\n",
    "                prev_actions = [a[-1] for a in dialogue.pop(-1).split(\"+\")]\n",
    "                actions = prev_actions + actions\n",
    "\n",
    "            dialogue.append(\n",
    "                \"+\".join(\n",
    "                    [\n",
    "                        f\"{utterance['resource']}_{action[0]}\"\n",
    "                        for action in np.unique(actions)\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    dialogues = list(filter(None, dialogues))\n",
    "    return dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_probabilities(dialogues: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Get transition probabilities for a list of dialogues.\n",
    "\n",
    "    Args:\n",
    "        dialogues: Dialogues where each dialogue is a string of actions.\n",
    "\n",
    "    Returns:\n",
    "        Transition probabilities for each action in the dialogues.\n",
    "    \"\"\"\n",
    "    transitions = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        for i in range(len(dialogue) - 1):\n",
    "            current_action = dialogue[i]\n",
    "            next_action = dialogue[i + 1]\n",
    "            if i == 0:\n",
    "                transitions[\"Start\"][current_action] += 1\n",
    "\n",
    "            transitions[current_action][next_action] += 1\n",
    "\n",
    "        transitions[dialogue[-1]][\"End\"] += 1\n",
    "\n",
    "    probabilities = {}\n",
    "    for action in transitions.keys():\n",
    "        total = sum(transitions[action].values())\n",
    "        if total > 0:\n",
    "            probabilities[action] = {\n",
    "                next_action: count / total\n",
    "                for next_action, count in transitions[action].items()\n",
    "            }\n",
    "        else:\n",
    "            probabilities[action] = {}\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def get_participants_transition_probs(\n",
    "    transition_probs: Dict[str, float]\n",
    ") -> Tuple[ParticipantTransitionProbs, ParticipantTransitionProbs]:\n",
    "    \"\"\"Gets the transitions probabilities for each participant.\n",
    "\n",
    "    Args:\n",
    "        transition_probs: Transition probabilities for all actions.\n",
    "\n",
    "    Returns:\n",
    "        Transition probabilities for each participant.\n",
    "    \"\"\"\n",
    "    user_transition_probs = {}\n",
    "    agent_transition_probs = {}\n",
    "    for state, transition in transition_probs.items():\n",
    "        if state.startswith(\"U_\"):\n",
    "            agent_transition_probs[state] = transition\n",
    "        elif state.startswith(\"S_\"):\n",
    "            user_transition_probs[state] = transition\n",
    "\n",
    "    return user_transition_probs, agent_transition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_POPULATIONS = {}\n",
    "AGENT_POPULATIONS = {}\n",
    "\n",
    "datasets = [\n",
    "    # (\"U1\", \"A1\", \"data/annotated_datasets/1_dstc1_updated.csv\"),\n",
    "    (\"U2\", \"A2\", \"data/annotated_datasets/2_dstc2_updated.csv\"),\n",
    "    (\"U3\", \"A3\", \"data/annotated_datasets/5_ode_updated.csv\"),\n",
    "    (\"U4\", \"A4\", \"data/annotated_datasets/4_scs_updated.csv\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/annotated_datasets/2_dstc2_updated.csv\n",
      "Processing data/annotated_datasets/5_ode_updated.csv\n",
      "Processing data/annotated_datasets/4_scs_updated.csv\n"
     ]
    }
   ],
   "source": [
    "data_stats = {}\n",
    "\n",
    "for user_pop, agent, path in datasets:\n",
    "    print(f\"Processing {path}\")\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.dropna(subset=[\"new\"])\n",
    "    dialogues = preprocess_dialogues(data)\n",
    "\n",
    "    # Compute statistics on the dialogues: avg. # utterance and std dev\n",
    "    num_utterances = [len(dialogue) for dialogue in dialogues]\n",
    "    data_stats[f\"D({user_pop}, {agent})\"] = {\n",
    "        \"# dialogues\": len(dialogues),\n",
    "        \"Avg. # utterances\": mean(num_utterances),\n",
    "        \"Std. dev. # utterances\": stdev(num_utterances),\n",
    "    }\n",
    "\n",
    "    transition_probabilities = get_transition_probabilities(dialogues)\n",
    "    user_transition_probs, agent_transition_probs = (\n",
    "        get_participants_transition_probs(transition_probabilities)\n",
    "    )\n",
    "\n",
    "    population = UserPopulation(user_pop, user_transition_probs)\n",
    "    population.add_historical_dialogues(dialogues)\n",
    "    USER_POPULATIONS[user_pop] = population\n",
    "\n",
    "    AGENT_POPULATIONS[agent] = {\n",
    "        \"transition_probabilities\": agent_transition_probs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dialogues statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_632fa\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_632fa_level0_col0\" class=\"col_heading level0 col0\" ># dialogues</th>\n",
       "      <th id=\"T_632fa_level0_col1\" class=\"col_heading level0 col1\" >Avg. # utterances</th>\n",
       "      <th id=\"T_632fa_level0_col2\" class=\"col_heading level0 col2\" >Std. dev. # utterances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_632fa_level0_row0\" class=\"row_heading level0 row0\" >D(U2, A2)</th>\n",
       "      <td id=\"T_632fa_row0_col0\" class=\"data row0 col0\" >2117.000</td>\n",
       "      <td id=\"T_632fa_row0_col1\" class=\"data row0 col1\" >10.171</td>\n",
       "      <td id=\"T_632fa_row0_col2\" class=\"data row0 col2\" >4.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_632fa_level0_row1\" class=\"row_heading level0 row1\" >D(U3, A3)</th>\n",
       "      <td id=\"T_632fa_row1_col0\" class=\"data row1 col0\" >25.000</td>\n",
       "      <td id=\"T_632fa_row1_col1\" class=\"data row1 col1\" >15.000</td>\n",
       "      <td id=\"T_632fa_row1_col2\" class=\"data row1 col2\" >8.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_632fa_level0_row2\" class=\"row_heading level0 row2\" >D(U4, A4)</th>\n",
       "      <td id=\"T_632fa_row2_col0\" class=\"data row2 col0\" >38.000</td>\n",
       "      <td id=\"T_632fa_row2_col1\" class=\"data row2 col1\" >1.579</td>\n",
       "      <td id=\"T_632fa_row2_col2\" class=\"data row2 col2\" >0.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x11010d0a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data_stats).transpose().style.format(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, dialogues, num_utterances, transition_probabilities, user_transition_probs, agent_transition_probs, data_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of success predictors\n",
    "\n",
    "In this part, we train a success predictor for each user population. It is a binary classifier that predicts the success of a dialogue based on the sequence of actions. The success predictor is based on a transformer model. Note that the success annotations are available for ODE and SCS datasets. For DSTC2, we assume that thank you messages indicate a successfule dialogue. For DSTC1, we manually annotate a subset of the dialogues to train the success predictor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_success_predictor(\n",
    "    train_data: pd.DataFrame,\n",
    "    test_data: pd.DataFrame,\n",
    "    output_dir: str,\n",
    "    class_weights: List[float] = None,\n",
    ") -> ClassificationModel:\n",
    "    \"\"\"Trains a success predictor model.\n",
    "\n",
    "    Args:\n",
    "        train_data: Training data.\n",
    "        test_data: Test data.\n",
    "        output_dir: Output directory to save the model.\n",
    "        class_weights: Class weights for the model. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Success predictor model.\n",
    "    \"\"\"\n",
    "    model_args = ClassificationArgs(\n",
    "        num_train_epochs=1, overwrite_output_dir=True\n",
    "    )\n",
    "    model = ClassificationModel(\n",
    "        \"distilbert\",\n",
    "        \"distilbert/distilbert-base-uncased\",\n",
    "        args=model_args,\n",
    "        use_cuda=False,\n",
    "        weight=class_weights,\n",
    "    )\n",
    "\n",
    "    model.train_model(train_data, output_dir=output_dir)\n",
    "\n",
    "    # Evaluate the model\n",
    "    result, _, _ = model.eval_model(test_data)\n",
    "    print(f\"Evaluation results: {result}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:610: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "4it [00:04,  1.13s/it]                       \n",
      "Epochs 1/1. Running Loss:    0.0019: 100%|██████████| 239/239 [02:17<00:00,  1.74it/s]\n",
      "Epoch 1 of 1: 100%|██████████| 1/1 [02:18<00:00, 138.22s/it]\n",
      "/Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:1453: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1it [00:37, 37.24s/it]\n",
      "Running Evaluation: 100%|██████████| 3/3 [00:03<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'mcc': 0.0, 'accuracy': 0.9528301886792453, 'f1_score': 0.0, 'tp': 0, 'tn': 202, 'fp': 0, 'fn': 10, 'auroc': 0.5683168316831683, 'auprc': 0.05917786066324769, 'eval_loss': 0.012262825776512424}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dstc2_data = pd.read_csv(\"data/annotated_datasets/2_dstc2_updated.csv\")\n",
    "u2_success_labels = (\n",
    "    dstc2_data.apply(\n",
    "        lambda x: 1 if \"be contented\" in x[\"Sitter\"] else 0, axis=1\n",
    "    )\n",
    "    .groupby(dstc2_data[\"case ID\"])\n",
    "    .max()\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    zip(\n",
    "        [\" \".join(d) for d in USER_POPULATIONS[\"U2\"].historical_dialogues],\n",
    "        u2_success_labels,\n",
    "    )\n",
    ")\n",
    "train_data, test_data = train_test_split(\n",
    "    data, test_size=0.1, random_state=123, stratify=data[1]\n",
    ")\n",
    "\n",
    "class_counts = data[1].value_counts()\n",
    "class_weights = [class_counts[0] / len(data), class_counts[1] / len(data)]\n",
    "output_dir = \"success_predictors/dstc2\"\n",
    "success_predictor = train_success_predictor(\n",
    "    train_data, test_data, output_dir=output_dir, class_weights=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_POPULATIONS[\"U2\"].set_success_predictor(success_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dstc2_data, u2_success_labels, data, train_data, test_data, class_counts, class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:610: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1it [00:03,  3.35s/it]\n",
      "Epochs 1/1. Running Loss:    0.5247: 100%|██████████| 3/3 [00:02<00:00,  1.37it/s]\n",
      "Epoch 1 of 1: 100%|██████████| 1/1 [00:03<00:00,  3.23s/it]\n",
      "/Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:1453: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1it [00:02,  2.98s/it]\n",
      "Running Evaluation: 100%|██████████| 1/1 [00:00<00:00, 12.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'mcc': 0.0, 'accuracy': 0.6666666666666666, 'f1_score': 0.8, 'tp': 2, 'tn': 0, 'fp': 1, 'fn': 0, 'auroc': 0.5, 'auprc': 0.8333333333333333, 'eval_loss': 0.462820440530777}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ode_data = pd.read_csv(\"data/annotated_datasets/5_ode_updated.csv\")\n",
    "u3_success_labels = (\n",
    "    ode_data.apply(\n",
    "        lambda x: 1 if x[\"activity name\"] == \"success()\" else 0, axis=1\n",
    "    )\n",
    "    .groupby(ode_data[\"case ID\"])\n",
    "    .max()\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    zip(\n",
    "        [\" \".join(d) for d in USER_POPULATIONS[\"U3\"].historical_dialogues],\n",
    "        u3_success_labels,\n",
    "    )\n",
    ")\n",
    "train_data, test_data = train_test_split(\n",
    "    data, test_size=0.1, random_state=123, stratify=data[1]\n",
    ")\n",
    "\n",
    "class_counts = data[1].value_counts()\n",
    "class_weights = [class_counts[0] / len(data), class_counts[1] / len(data)]\n",
    "output_dir = \"success_predictors/ode\"\n",
    "success_predictor = train_success_predictor(\n",
    "    train_data, test_data, output_dir=output_dir, class_weights=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_POPULATIONS[\"U3\"].set_success_predictor(success_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ode_data, u3_success_labels, data, train_data, test_data, class_counts, class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:610: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1it [00:02,  2.93s/it]\n",
      "Epochs 1/1. Running Loss:    0.6860: 100%|██████████| 5/5 [00:02<00:00,  1.89it/s]\n",
      "Epoch 1 of 1: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it]\n",
      "/Users/2925364/miniconda3/envs/sigir24/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:1453: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1it [00:02,  2.90s/it]\n",
      "Running Evaluation: 100%|██████████| 1/1 [00:00<00:00, 10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'mcc': 0.0, 'accuracy': 0.5, 'f1_score': 0.0, 'tp': 0, 'tn': 2, 'fp': 0, 'fn': 2, 'auroc': 0.25, 'auprc': 0.5, 'eval_loss': 0.7209495306015015}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "u4_success_labels = pd.read_csv(\"data/success_annotation/scs.csv\")\n",
    "u4_success_labels = u4_success_labels.apply(max, axis=1).to_list()\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    zip(\n",
    "        [\" \".join(d) for d in USER_POPULATIONS[\"U4\"].historical_dialogues],\n",
    "        u4_success_labels,\n",
    "    )\n",
    ")\n",
    "train_data, test_data = train_test_split(\n",
    "    data, test_size=0.1, random_state=123, stratify=data[1]\n",
    ")\n",
    "\n",
    "class_counts = data[1].value_counts()\n",
    "class_weights = [class_counts[0] / len(data), class_counts[1] / len(data)]\n",
    "output_dir = \"success_predictors/scs\"\n",
    "success_predictor = train_success_predictor(\n",
    "    train_data, test_data, output_dir=output_dir, class_weights=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_POPULATIONS[\"U4\"].set_success_predictor(success_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del u4_success_labels, data, train_data, test_data, class_counts, class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of synthetic dialogues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_action(\n",
    "    current_action: str, transition_probs: ParticipantTransitionProbs\n",
    ") -> str:\n",
    "    \"\"\"Samples the next action based on transition probabilities.\n",
    "\n",
    "    Args:\n",
    "        current_action: Current action.\n",
    "        transition_probs: Transition probabilities.\n",
    "\n",
    "    Returns:\n",
    "        Next action.\n",
    "    \"\"\"\n",
    "    next_actions = list(transition_probs[current_action].keys())\n",
    "    probabilities = list(transition_probs[current_action].values())\n",
    "    sampled_action = np.random.choice(next_actions, p=probabilities)\n",
    "    return sampled_action\n",
    "\n",
    "\n",
    "def sample_dialogue(\n",
    "    agent_transition_probs: ParticipantTransitionProbs,\n",
    "    user_transition_probs: ParticipantTransitionProbs,\n",
    ") -> List[str]:\n",
    "    \"\"\"Samples a dialogue.\n",
    "\n",
    "    Args:\n",
    "        agent_transition_probs: Transition probabilities for the agent.\n",
    "        user_transition_probs: Transition probabilities for the user.\n",
    "\n",
    "    Returns:\n",
    "        Dialogue as list of actions.\n",
    "    \"\"\"\n",
    "    dialogue = []\n",
    "    is_finished = False\n",
    "\n",
    "    current_action = random.choice(\n",
    "        list(agent_transition_probs.keys())\n",
    "        + list(user_transition_probs.keys())\n",
    "    )\n",
    "    dialogue.append(current_action)\n",
    "    while not is_finished:\n",
    "        try:\n",
    "            if current_action.startswith(\"U_\"):\n",
    "                current_action = sample_next_action(\n",
    "                    current_action, agent_transition_probs\n",
    "                )\n",
    "            else:\n",
    "                current_action = sample_next_action(\n",
    "                    current_action, user_transition_probs\n",
    "                )\n",
    "            if current_action == \"End\":\n",
    "                is_finished = True\n",
    "                break\n",
    "            dialogue.append(current_action)\n",
    "        except KeyError:\n",
    "            current_action = current_action.split(\"+\")[-1]\n",
    "\n",
    "    return dialogue\n",
    "\n",
    "\n",
    "def sample_dialogues(\n",
    "    agent_transition_probs: ParticipantTransitionProbs,\n",
    "    user_transition_probs: ParticipantTransitionProbs,\n",
    "    success_predictor: ClassificationModel,\n",
    "    num_dialogues: int,\n",
    ") -> List[Tuple[List[str], bool]]:\n",
    "    \"\"\"Samples dialogues.\n",
    "\n",
    "    Args:\n",
    "        agent_transition_probs: Transition probabilities for the agent.\n",
    "        user_transition_probs: Transition probabilities for the user.\n",
    "        success_predictor: Success predictor model.\n",
    "        num_dialogues: Number of dialogues to sample.\n",
    "\n",
    "    Returns:\n",
    "        Dialogues with success status.\n",
    "    \"\"\"\n",
    "    dialogues = []\n",
    "    for _ in range(num_dialogues):\n",
    "        dialogue = sample_dialogue(\n",
    "            agent_transition_probs, user_transition_probs\n",
    "        )\n",
    "        success = success_predictor.predict([\" \".join(dialogue)])[0][0]\n",
    "        dialogues.append((dialogue, success))\n",
    "\n",
    "    return dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "This part contains the methods to compute the metrics associated to the training and evaluation objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from rouge_score import rouge_scorer\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We choose to use Jensen-Shannon divergence (JSD) and ROUGE-L as metrics to assess the similarity between the user population and simulated user populations. These allow us to make an assessment at the utterance- and dialogue-level respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jsd(\n",
    "    user_policy: ParticipantTransitionProbs,\n",
    "    simulated_user_policy: ParticipantTransitionProbs,\n",
    ") -> float:\n",
    "    \"\"\"Computes Jensen-Shannon divergence between user and simulated user\n",
    "    policies.\n",
    "\n",
    "    It computes the Jensen-Shannon divergence between the transition\n",
    "    probabilities for each state and then averages them. Epsilon is added to \n",
    "    avoid division by zero.\n",
    "\n",
    "    Args:\n",
    "        user_policy: User policy.\n",
    "        simulated_user_policy: Simulated user policy.\n",
    "\n",
    "    Returns:\n",
    "        Jensen-Shannon divergence.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-9\n",
    "    total_jsd = 0.0\n",
    "    for state, transitions_probabilities in user_policy.items():\n",
    "        # Add epsilon to avoid division by zero\n",
    "        simulated_user_policy[state] = {\n",
    "            k: simulated_user_policy.get(state, {}).get(k, epsilon) for k in transitions_probabilities.keys()\n",
    "        }\n",
    "\n",
    "        probabilities = np.array(list(transitions_probabilities.values()))\n",
    "        simulated_probabilities = np.array(\n",
    "            list(simulated_user_policy[state].values())\n",
    "        )\n",
    "            \n",
    "        total_jsd += distance.jensenshannon(\n",
    "            probabilities, simulated_probabilities, base=2\n",
    "        )\n",
    "    return total_jsd / len(user_policy.keys())\n",
    "\n",
    "\n",
    "def compute_rouge_score(\n",
    "    historical_dialogues: List[List[str]], simulated_dialogues: List[List[str]]\n",
    ") -> float:\n",
    "    \"\"\"Computes ROUGE-L score between historical and simulated dialogues.\n",
    "\n",
    "    It computes the average ROUGE-L score between all pairs of historical and\n",
    "    simulated dialogues.\n",
    "\n",
    "    Args:\n",
    "        historical_dialogues: Historical dialogues.\n",
    "        simulated_dialogues: Simulated dialogues.\n",
    "\n",
    "    Returns:\n",
    "        ROUGE-L score.\n",
    "    \"\"\"\n",
    "    historical_dialogues = [\" \".join(d) for d in historical_dialogues]\n",
    "    simulated_dialogues = [\" \".join(d) for d in simulated_dialogues]\n",
    "    total_score = 0.0\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"])\n",
    "    pairs = list(product(historical_dialogues, simulated_dialogues))\n",
    "    for h, s in pairs:\n",
    "        total_score += scorer.score(h, s)[\"rougeL\"].fmeasure\n",
    "    return total_score / len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We use the success rate as the performance metric to evaluate the conversational agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_success_rate(successes: List[int]) -> float:\n",
    "    \"\"\"Computes success rate.\n",
    "\n",
    "    Args:\n",
    "        successes: Successes.\n",
    "\n",
    "    Returns:\n",
    "        Success rate.\n",
    "    \"\"\"\n",
    "    return sum(successes) / len(successes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out cross-validation\n",
    "\n",
    "In this part, we perform a leave-out-one out experiment to answer the following questions: is the optimal user simulator for training also the best for evaluation, and vice versa?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# participant_pairs = [(\"U1\", \"A1\"), (\"U2\", \"A2\"), (\"U3\", \"A3\"), (\"U4\", \"A4\")]\n",
    "participant_pairs = [(\"U2\", \"A2\"), (\"U3\", \"A3\"), (\"U4\", \"A4\")]\n",
    "num_synthetic_dialogues = 100\n",
    "\n",
    "results = defaultdict(dict)\n",
    "\n",
    "for user_pop, agent in participant_pairs:\n",
    "    print(f\"Reference - {user_pop}, {agent}\")\n",
    "    user_population = USER_POPULATIONS[user_pop]\n",
    "    for _, simulated_user_population in USER_POPULATIONS.items():\n",
    "        if user_pop == simulated_user_population.name:\n",
    "            continue\n",
    "\n",
    "        # Generate synthetic dialogues\n",
    "        synthetic_dialogues_data = sample_dialogues(\n",
    "            AGENT_POPULATIONS[agent][\"transition_probabilities\"],\n",
    "            simulated_user_population.transition_probabilities,\n",
    "            simulated_user_population.success_predictor,\n",
    "            num_synthetic_dialogues,\n",
    "        )\n",
    "\n",
    "        simulated_dialogues = []\n",
    "        simulated_dialogues_success = []\n",
    "        for dialogue, success in synthetic_dialogues_data:\n",
    "            simulated_dialogues.append(dialogue)\n",
    "            simulated_dialogues_success.append(success)\n",
    "\n",
    "        # Compute ROUGE-L score\n",
    "        rouge_l_score = compute_rouge_score(\n",
    "            user_population.historical_dialogues,\n",
    "            simulated_dialogues,\n",
    "        )\n",
    "\n",
    "        # Compute success rate\n",
    "        success_rate = compute_success_rate(simulated_dialogues_success)\n",
    "\n",
    "        results[f\"{user_pop}, {agent}\"][simulated_user_population.name] = {\n",
    "            \"ROUGE-L\": rouge_l_score,\n",
    "            \"Success rate\": success_rate,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_69702\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_69702_level0_col0\" class=\"col_heading level0 col0\" >ROUGE-L</th>\n",
       "      <th id=\"T_69702_level0_col1\" class=\"col_heading level0 col1\" >Success rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Reference</th>\n",
       "      <th class=\"index_name level1\" >Simulated user pop.</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_69702_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"2\">U2, A2</th>\n",
       "      <th id=\"T_69702_level1_row0\" class=\"row_heading level1 row0\" >U3</th>\n",
       "      <td id=\"T_69702_row0_col0\" class=\"data row0 col0\" >0.523</td>\n",
       "      <td id=\"T_69702_row0_col1\" class=\"data row0 col1\" >1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69702_level1_row1\" class=\"row_heading level1 row1\" >U4</th>\n",
       "      <td id=\"T_69702_row1_col0\" class=\"data row1 col0\" >0.448</td>\n",
       "      <td id=\"T_69702_row1_col1\" class=\"data row1 col1\" >0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69702_level0_row2\" class=\"row_heading level0 row2\" rowspan=\"2\">U3, A3</th>\n",
       "      <th id=\"T_69702_level1_row2\" class=\"row_heading level1 row2\" >U2</th>\n",
       "      <td id=\"T_69702_row2_col0\" class=\"data row2 col0\" >0.523</td>\n",
       "      <td id=\"T_69702_row2_col1\" class=\"data row2 col1\" >0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69702_level1_row3\" class=\"row_heading level1 row3\" >U4</th>\n",
       "      <td id=\"T_69702_row3_col0\" class=\"data row3 col0\" >0.426</td>\n",
       "      <td id=\"T_69702_row3_col1\" class=\"data row3 col1\" >0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69702_level0_row4\" class=\"row_heading level0 row4\" rowspan=\"2\">U4, A4</th>\n",
       "      <th id=\"T_69702_level1_row4\" class=\"row_heading level1 row4\" >U2</th>\n",
       "      <td id=\"T_69702_row4_col0\" class=\"data row4 col0\" >0.527</td>\n",
       "      <td id=\"T_69702_row4_col1\" class=\"data row4 col1\" >0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69702_level1_row5\" class=\"row_heading level1 row5\" >U3</th>\n",
       "      <td id=\"T_69702_row5_col0\" class=\"data row5 col0\" >0.445</td>\n",
       "      <td id=\"T_69702_row5_col1\" class=\"data row5 col1\" >1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2bd81c700>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for participant_pair, d in results.items():\n",
    "    for simulated_user, metrics in d.items():\n",
    "        rows.append((participant_pair, simulated_user, *(value for _, value in metrics.items())))\n",
    "\n",
    "summary = pd.DataFrame(rows, columns=[\"Reference\", \"Simulated user pop.\",\"ROUGE-L\", \"Success rate\"])\n",
    "summary.set_index([\"Reference\", \"Simulated user pop.\"], inplace=True)\n",
    "\n",
    "summary.style.format(precision=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jensen-Shannon divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsd_results = defaultdict(dict)\n",
    "\n",
    "for user_pop1, user_pop2 in product(USER_POPULATIONS.keys(), repeat=2):\n",
    "    if user_pop1 != user_pop2:\n",
    "        user_policy1 = USER_POPULATIONS[user_pop1].transition_probabilities\n",
    "        user_policy2 = USER_POPULATIONS[user_pop2].transition_probabilities\n",
    "        jsd = compute_jsd(user_policy1, user_policy2)\n",
    "        jsd_results[user_pop1][user_pop2] = jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_611b1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_611b1_level0_col0\" class=\"col_heading level0 col0\" >U2</th>\n",
       "      <th id=\"T_611b1_level0_col1\" class=\"col_heading level0 col1\" >U3</th>\n",
       "      <th id=\"T_611b1_level0_col2\" class=\"col_heading level0 col2\" >U4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_611b1_level0_row0\" class=\"row_heading level0 row0\" >U2</th>\n",
       "      <td id=\"T_611b1_row0_col0\" class=\"data row0 col0\" >nan</td>\n",
       "      <td id=\"T_611b1_row0_col1\" class=\"data row0 col1\" >0.412</td>\n",
       "      <td id=\"T_611b1_row0_col2\" class=\"data row0 col2\" >0.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_611b1_level0_row1\" class=\"row_heading level0 row1\" >U3</th>\n",
       "      <td id=\"T_611b1_row1_col0\" class=\"data row1 col0\" >0.412</td>\n",
       "      <td id=\"T_611b1_row1_col1\" class=\"data row1 col1\" >nan</td>\n",
       "      <td id=\"T_611b1_row1_col2\" class=\"data row1 col2\" >0.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_611b1_level0_row2\" class=\"row_heading level0 row2\" >U4</th>\n",
       "      <td id=\"T_611b1_row2_col0\" class=\"data row2 col0\" >0.383</td>\n",
       "      <td id=\"T_611b1_row2_col1\" class=\"data row2 col1\" >0.330</td>\n",
       "      <td id=\"T_611b1_row2_col2\" class=\"data row2 col2\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2bd83d130>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(jsd_results).sort_index().style.format(precision=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigir24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
